{"cells":[{"cell_type":"markdown","metadata":{"id":"RzMsP9hsc36l"},"source":["## numpy와 닮은 파이토치(pytorch)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1672662359742,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"SAmMX78Lc3Iy","outputId":"e02bea79-5c95-428c-f715-2a30bd325221"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3, 4])\n","<class 'torch.Tensor'>\n","torch.int64\n","torch.Size([4])\n","torch.float32\n","tensor([1.0000, 2.0000, 3.1000, 4.0000])\n"]}],"source":["import torch\n","a=torch.tensor([1,2,3,4])\n","print(a)\n","print(type(a))\n","print(a.dtype) # data type -> 안에 있는 것의 자료형(여기선 1,2,3,4의 자료형을 의미)\n","print(a.shape) # print(a.size())랑 같은 결과\n","b=torch.tensor([1,2, 3.1   , 4])\n","print(b.dtype) # 하나라도 실수면 자동으로 실수 타입으로\n","print(b)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"elapsed":19,"status":"error","timestamp":1674645541231,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"QRPA6o3EdDy5","outputId":"46dab2fd-e751-4566-eb91-d55544f4a597"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2, 3],\n","        [3, 4, 5]])\n","torch.Size([2, 3])\n","2\n","6\n"]}],"source":["A=torch.tensor([[1,2,3],[3,4,5]]) \n","# A=torch.tensor([[1,2],[3,4,5]]) # 리스트와는 달리 이제는 행렬이라서 각 행에 해당하는 숫자의 개수가 같아야 함\n","print(A)\n","print(A.shape)\n","print(A.ndim) # 차원의 수 \n","print(A.numel()) # 전체 성분의 수"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1672662359743,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"DtQ20sP8dv8i","outputId":"0fa3fe4c-9db7-4030-a262-59c6f6e335b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0., 0., 0., 0., 0.])\n","tensor([[0, 0, 0],\n","        [0, 0, 0]])\n","tensor([1., 1., 1., 1., 1.])\n","tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([3, 5, 7, 9])\n","tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n","        0.9000])\n","tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n","        1.0000])\n"]}],"source":["print(torch.zeros(5))\n","print(torch.zeros_like(A)) # A랑 같은 shape의 행렬을 만들어준다\n","print(torch.ones(5))\n","print(torch.zeros(3,3))\n","print(torch.arange(3,10,2)) # range랑 같은데 tensor로 만들어줌 , \n","print(torch.arange(0,1,0.1)) # 소수점 가능\n","print(torch.linspace(0,1,10)) # 0 에서부터 1 포함(default) 10개로"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1672662359743,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"mEY_M0PPfRCz","outputId":"0b10f4e0-55ee-472f-aa71-0fafed6cde43"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([5, 7, 9])\n"]}],"source":["a=torch.tensor([1,2,3])\n","b=torch.tensor([4,5,6])\n","c=a+b\n","print(c) # 드디어! 양 옆으로 길어지는 게 아닌 벡터의 합이 나옴! ->  numpy의 덧셈과 다르다."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1672662359744,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"zRGH_ASMfY5q","outputId":"c5d407f0-2a5a-4782-c805-d57ec6d24f97"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[5, 7, 9],\n","        [2, 3, 4]])\n","tensor([[-3, -3, -3],\n","        [ 0,  1,  2]])\n","\n","tensor([[ 4, 10, 18],\n","        [ 1,  2,  3]])\n","tensor([[0.2500, 0.4000, 0.5000],\n","        [1.0000, 2.0000, 3.0000]])\n","tensor([[16, 25, 36],\n","        [ 1,  1,  1]])\n"]}],"source":["A=torch.tensor([[1,2,3],[1,2,3]])\n","B=torch.tensor([[4,5,6],[1,1,1]])\n","C=A+B\n","D=A-B\n","print(C) \n","print(D)\n","print()\n","print(A*B) # 곱셈은? 성분끼리의 곱! (Hadamard product) -> 실제 행렬의 곱과는 다르다.\n","print(A/B) # 나누기도 마찬가지\n","print(B**2) # 제곱도 각 성분에 대해서 해준다 -> 실제 행렬의 곱과는 다르다."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1672662359744,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"LDMR3HebgeIS","outputId":"d4a1ec00-300d-4e20-c59a-3bb6cd17acb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 1,  4],\n","        [ 9, 16]])\n","tensor([[ 7, 10],\n","        [15, 22]])\n"]}],"source":["A=torch.tensor([[1,2],[3,4]])\n","B=torch.tensor([[1,2],[3,4]])\n","print(A*B)\n","print(A@B) # 이게 진짜 행렬 곱하기"]},{"cell_type":"markdown","metadata":{"id":"lwIW_G6ThRuW"},"source":["## pytorch 의 인덱싱 슬라이스"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1672662359744,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"EvEXUodtlOyq","outputId":"837b92dc-420e-4980-f04a-ce19a99d45de"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(1)\n","tensor(2)\n","tensor(9)\n","tensor([2, 3, 4])\n","tensor([8, 9])\n","tensor([1, 2, 3, 4, 5, 6, 7])\n","tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}],"source":["a=torch.tensor([1,2,3,4,5,6,7,8,9]) \n","# 인덱싱과 슬라이싱, list할 때와 동일!\n","print(a[0]) \n","print(a[1])\n","print(a[-1])\n","print(a[1:4])\n","print(a[7:])\n","print(a[:7])\n","print(a[:])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1672662359744,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"4PSLeE0flTMC","outputId":"2cfd56d8-b2f3-450e-cd62-703fe99c62df"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3])\n","tensor([7, 8, 9])\n","tensor([[4, 5, 6],\n","        [7, 8, 9]])\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","tensor(3)\n","tensor(3)\n","[[1, 2, 3, 4], [5, 6, 7, 8]]\n","3\n","----------------------------\n","tensor([4, 5, 6])\n","tensor([4, 6])\n","tensor([3, 6, 9])\n","tensor([7, 8, 9])\n"]}],"source":["# 행렬에 대한 인덱싱과 슬라이싱\n","A=torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n","print(A[0]) # 하나만 쓰면 '행'에 대한 인덱싱 (리스트 속 리스트 생각)\n","print(A[-1])\n","print(A[1:])\n","print(A[:])\n","print(A[0][2])\n","print(A[0,2]) # ->여기서는 A[0][2]와 같은 결과가 나오지만 다른 case에서는 다른 결과 나올수있다.\n","              # (작동방식이 다르다. -> 밑에 중요!!부분 보기) \n","              # 리스트와 달리 이런 것도 됨\n","\n","B=[[1,2,3,4], [5,6,7,8]]\n","print(B)\n","print(B[0][2])\n","# print(B[0,2]) # error! -> 리스트에선 B[~,~] 슬라이싱 불가\n","\n","print(A[1,:]) # 1 행, 전부 , A[a,b] -> a => a번째 행, b => b번째 열 -> 핵심 포인트!!!!\n","print(A[1,0:3:2])\n","\n","# 중요!!(여기서 A[:,2]와 A[:][2]는 다른결과)---------------------------------------\n","print(A[:,2]) # 전부, 2 열\n","print(A[:][2])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1672662359745,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"y6Zg1lZ9ZIBe","outputId":"665f32fe-663e-4340-b699-82a108d74a06"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 0,  1,  2,  3],\n","         [ 4,  5,  6,  7],\n","         [ 8,  9, 10, 11]],\n","\n","        [[12, 13, 14, 15],\n","         [16, 17, 18, 19],\n","         [20, 21, 22, 23]]])\n","tensor(6)\n","torch.Size([])\n","torch.Size([4])\n","torch.Size([1, 4])\n","torch.Size([1, 1, 4])\n"]}],"source":["# 3차원 행렬 인덱싱\n","A=torch.tensor([ [[0,1,2,3],[4,5,6,7],[8,9,10,11]] ,   \n","             [[12,13,14,15],[16,17,18,19],[20,21,22,23]] ])\n","print(A)\n","print(A[0,1,2]) # A[a,b,c] -> a => a번째 채널 , b => b번째 행, c => c번째 열\n","\n","a=torch.tensor(1) # 0차원\n","a_1=torch.tensor([1,2,3,4]) # 1D tensor -> 행렬도 아니고 벡터도 아니다.\n","a_2 = torch.tensor([[1,2,3,4]])\n","a_3 = torch.tensor([[[1,2,3,4]]])\n","print(a.shape)\n","print(a_1.shape)\n","print(a_2.shape)\n","print(a_3.shape) # 대괄호가 하나 늘어나면 왼쪽에 shape 값이 추가 된다!"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1672662360158,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"DBjH6loolzdS","outputId":"d0f51ebd-6e61-4e56-9b74-f5849a73b683"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","tensor([[False, False,  True, False],\n","        [False,  True, False,  True]])\n","tensor([3, 3, 3])\n","tensor([[  1,   2, 100,   4],\n","        [  5, 100,   7, 100]])\n","tensor([[1, 2],\n","        [7, 8]])\n","tensor([1, 2])\n"]}],"source":["# boolean 인덱싱\n","a=[1,2,3,4,5,3,3]\n","print(a==3) # 여러개 값 들어있는 리스트랑 3 달랑 하나랑 같냐? 다르다!\n","A=torch.tensor([[1,2,3,4],[5,3,7,3]])\n","print(A==3) # 리스트와 달리 각 성분에 대해 비교해줌\n","print(A[A==3]) # True, False가 담긴 행렬로 인덱싱 가능!!\n","\n","A[A==3] = 100\n","print(A) # 그러면 이런 것도 가능하다! (3과 같은 애를 100으로 바꿔줘)\n","\n","A=torch.tensor([[1,2],[3,4],[5,6],[7,8]])\n","B=torch.tensor([True, False, False, True]) # 참고로 그냥 리스트여도 됨\n","print(A[B,:]) # 0행, 3행 슬라이싱 -> A[A==3]에서는 A와 A==3의 형태가 같았지만 여기선 행,열을 따로 생각해 해준다. \n","\n","b=torch.tensor([1,2,3,4])\n","print(b[[True,True,False,False]])\n","c=[1,2,3,4]\n","# c[[True,True,False,False]] # error! -> 그냥 리스트에선 안된다."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62,"status":"ok","timestamp":1672662360158,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"FzKn6wIJZAgn","outputId":"3b426fa8-3d33-43ca-8964-70bef0dcc416"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(3)\n","tensor(3)\n","tensor([3, 4, 5])\n","tensor([[3, 3, 3],\n","        [4, 4, 4]])\n","tensor([1, 2, 3])\n","torch.Size([2, 2, 3])\n","tensor([[[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[4, 5, 6],\n","         [4, 5, 6]]])\n"]}],"source":["# tensor로 인덱싱\n","a=torch.tensor([1,2,3,4,5])\n","A=a[2]\n","print(A)\n","A=a[ torch.tensor(2) ] # torch.tensor를 안에다가?\n","print(A)\n","\n","A=a[ torch.tensor([2,3,4]) ] # -> a의 2번째,3번째4번째를 가져온다.\n","print(A)\n","A=a[ torch.tensor([[2,2,2],[3,3,3]]) ] # -> shape에 맞게 값을 가져온다 -> 인덱싱된 애들로 2행 3열짜리 행렬을 만든다 \n","print(A)\n","\n","a=[1,2,3]\n","# a[ [1,1,1,1,2,2,2] ] # error!\n","\n","# 주의깊게 보기!!\n","a=torch.tensor([[1,2,3],[4,5,6]])\n","print(a[0])\n","A=a[ torch.tensor([[0,1],[1,1]]) ]\n","\n","print(A.shape) # 예를 들어, a[0] = tensor([1,2,3])과 같이 1차원 데이터이므로 한 차원이 뒤에 늘어나서 2,2, \"3\" 이 된다!\n","print(A) # segmentation 결과 그림 보여줄 때 사용!"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1672662360159,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"BBEF3i6xl1sK","outputId":"70f48350-1c6d-41f3-c0c7-6d95e4259b03"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n","torch.Size([4, 2])\n","tensor(2)\n","tensor([2])\n","tensor([2])\n","tensor([2])\n","tensor([[3, 4],\n","        [3, 4],\n","        [5, 6],\n","        [5, 6],\n","        [5, 6]])\n"]}],"source":["A=torch.tensor([[1,2],[3,4],[5,6],[7,8]])\n","print(A)\n","print(A.shape)\n","\n","# 1. A[몇 번째 행이냐, 몇 번째 열이냐]\n","print(A[0,1])\n","# 2. torch.tensor(bool) ] => A와 같은 shape을 가지는 tensor형 bool이 어디에 True를 가지고 있냐\n","print(A[ torch.tensor([[False,True],[False,False],[False,False],[False,False]]) ])\n","print(A[A==2])\n","# 3. A[몇 번째 값에 True가 있냐, 몇 번째 값에 True가 있냐] -----------------------> 이렇게도 사용이 가능하다.\n","print(A[ [True,False,False,False], [False,True] ])\n","# 4. torch.tensor ] # 몇 번째 것을 어떻게 쌓을거냐\n","print(A[ torch.tensor([1,1,2,2,2]) ])"]},{"cell_type":"markdown","metadata":{"id":"QYBXKfCupHU9"},"source":["## pytorch의 여러 함수들"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1672662360159,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"0lfFqRGOpI-0","outputId":"6e64dfdf-5d33-489b-db14-6c4bdada5ecf"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.5087, -0.3277,  1.5919],\n","        [ 1.7668, -1.6455,  0.9617],\n","        [-0.2354, -0.1724,  0.0155]])\n","tensor([[0.3362, 0.8400, 0.0632],\n","        [0.6985, 0.5289, 0.3308],\n","        [0.4202, 0.5947, 0.1670]])\n","tensor([[-0.2354, -0.1724,  0.0155]])\n"]}],"source":["A=torch.randn(3,3) # Normal 의 n , (평균0 ,분산 1) -> if 평균 10으로 하고싶으면 +10해주기, 분산100해주고 싶으면 10을 곱하기(var(x) = 1, var(10*x)=10*10*var(x)=100)\n","B=torch.rand(3,3)  # 이건 uniform,  (0부터 1사이) -> if -5부터 5사이 하고싶으면 -5해주기 \n","print(A)\n","print(B)\n","print(A[A[:,0]<0,:]) # 0 번째 열이 음수인 행들"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1672662360159,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"J6KxUXALpniU","outputId":"fb2fa9d4-4482-4936-c9d7-c628a5c6a049"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0.1671,  0.6493, -1.0987],\n","        [-0.0301, -0.0827, -0.2128],\n","        [-0.2929, -0.3184, -0.4005]])\n","tensor([[0.1671, 0.6493, 1.0987],\n","        [0.0301, 0.0827, 0.2128],\n","        [0.2929, 0.3184, 0.4005]])\n","tensor([[0.4087, 0.8058, 1.0482],\n","        [0.1734, 0.2875, 0.4613],\n","        [0.5412, 0.5643, 0.6328]])\n","tensor([[1.1818, 1.9141, 0.3333],\n","        [0.9704, 0.9207, 0.8083],\n","        [0.7461, 0.7273, 0.6700]])\n","tensor([[-1.7893, -0.4319,  0.0941],\n","        [-3.5047, -2.4929, -1.5472],\n","        [-1.2281, -1.1443, -0.9151]])\n","tensor(1.0000)\n","tensor(1.)\n","tensor(1.)\n","tensor([[ 0.,  1., -1.],\n","        [-0., -0., -0.],\n","        [-0., -0., -0.]])\n","tensor([[ 0.1700,  0.6500, -1.1000],\n","        [-0.0300, -0.0800, -0.2100],\n","        [-0.2900, -0.3200, -0.4000]])\n","tensor([[ 0.,  0., -2.],\n","        [-1., -1., -1.],\n","        [-1., -1., -1.]])\n","tensor([[ 1.,  1., -1.],\n","        [-0., -0., -0.],\n","        [-0., -0., -0.]])\n"]}],"source":["A=torch.randn(3,3)\n","print(A)\n","print(torch.abs(A))\n","print(torch.sqrt(torch.abs(A)))\n","print(torch.exp(A)) # ex) A=[[1,2],[3,4]] , torch.exp(A) = [[e^1,e^2],[e^3,e^4]] -> 각각에 대해서 적용\n","print(torch.log(torch.abs(A)))\n","print(torch.log(torch.exp(torch.tensor(1)))) # torch.exp(torch.tensor(1)) = e^1, torch.tensor(1)가 아닌 1이 들어가면 에러\n","print(torch.log10(torch.tensor(10)))\n","print(torch.log2(torch.tensor(2)))\n","print(torch.round(A)) # 반올림\n","print(torch.round(A,decimals = 2)) # 소수점 둘째자리까지\n","print(torch.floor(A)) # 내림\n","print(torch.ceil(A)) # 올림"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1672662360160,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"PFPjWX54qaXi","outputId":"8ad58a56-7781-499d-d499-00029b177e3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.5000)\n","tensor(0.5000)\n","tensor(1.)\n","tensor(-1.)\n","<class 'float'>\n","<class 'torch.Tensor'>\n"]}],"source":["print(torch.sin(torch.tensor(torch.pi/6)))\n","print(torch.cos(torch.tensor(torch.pi/3)))\n","print(torch.tan(torch.tensor(torch.pi/4)))\n","print(torch.tanh(torch.tensor(-10)))\n","\n","# type(torch.pi) -> float임 , 만약 tensor와 연산하면 tensor로 바뀜\n","print(type(torch.pi/3))\n","print(type(torch.tensor(3)/3))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1672662360160,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"huDfIsJoqdDa","outputId":"cd2caa43-010a-43c8-d6df-b935da479861"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(nan)\n","tensor([False, False,  True, False, False])\n","tensor([False, False, False, False,  True])\n"]}],"source":["torch.nan # not a number\n","print(torch.log(torch.tensor(-1)))\n","\n","print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))\n","print(torch.isinf(torch.tensor([1,2,3,4,torch.inf]))) # torch.inf -> torch infinite (무한대)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[100,   2,   3,   4],\n","         [  5,   6,   7,   8],\n","         [  1,  41,   6,  12]],\n","\n","        [[ 32,  56,  10,   0],\n","         [ 24,  17,   3,   9],\n","         [  5,   4,  61,  13]]])\n"]}],"source":["# dim\n","import torch\n","A=torch.tensor([[[100,2,3,4],[5,6,7,8],[1,41,6,12]],[[32,56,10,0],[24,17,3,9],[5,4,61,13]]])\n","print(A)\n","# print(A.shape) # 결과 : torch.Size([2, 3, 4])\n","\n","# print(A.max(dim=0))\n","# print(A.max(dim=1))\n","# print(A.max(dim=2))\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1672662360160,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"C9G0Rdhiqn3a","outputId":"82abc814-a38e-4545-af25-da0bbba5d432"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.0316, -0.1095, -0.4702,  2.3332],\n","        [-1.7127, -0.3309,  0.3516, -0.1013],\n","        [-0.3014, -0.6416,  0.0091,  0.3984]])\n","tensor(2.3332)\n","torch.return_types.max(\n","values=tensor([-0.0316, -0.1095,  0.3516,  2.3332]),\n","indices=tensor([0, 0, 1, 0]))\n","torch.return_types.max(\n","values=tensor([2.3332, 0.3516, 0.3984]),\n","indices=tensor([3, 2, 3]))\n","torch.return_types.max(\n","values=tensor([[-0.0316, -0.1095,  0.3516,  2.3332]]),\n","indices=tensor([[0, 0, 1, 0]]))\n","torch.return_types.max(\n","values=tensor([[2.3332],\n","        [0.3516],\n","        [0.3984]]),\n","indices=tensor([[3],\n","        [2],\n","        [3]]))\n","tensor(-1.7127)\n","torch.return_types.min(\n","values=tensor([-1.7127, -0.6416, -0.4702, -0.1013]),\n","indices=tensor([1, 2, 0, 1]))\n","torch.return_types.min(\n","values=tensor([-0.4702, -1.7127, -0.6416]),\n","indices=tensor([2, 0, 1]))\n","tensor(3)\n","tensor([0, 0, 1, 0])\n","tensor([3, 2, 3])\n"]}],"source":["A=torch.randn(3,4)\n","print(A)\n","print(torch.max(A)) # a.max()와 같은 결과(-> a.max(dim=0)형태를 더 많이 쓴다)\n","print(torch.max(A,dim=0)) # -> dim은 numpy에서의 axis , 0번째 dim은 3, 1번째 dim은 4, 여기선 3개중 최댓값 1개를 골라라\n","print(torch.max(A,dim=1)) # 1D array로 바꿔버림\n","print(torch.max(A,dim=0, keepdims=True)) # keepdims -> 기존 차원 유지\n","print(torch.max(A,dim=1, keepdims=True)) # 3 행 1열 짜리 2D array\n","print(torch.min(A))\n","print(torch.min(A,dim=0))\n","print(torch.min(A,dim=1))\n","print(torch.argmax(A)) # -> 인덱스 반환\n","print(torch.argmax(A,dim=0)) # 각 열에서 가장 큰 애가 존재하는 인덱스\n","print(torch.argmax(A,dim=1)) # 각 행에서 가장 큰 애가 존재하는 인덱스"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1672662360161,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"5h8JeQ8bqsrv","outputId":"0cf370c9-1cf9-4cda-83ff-b7af887a9745"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.4611],\n","        [ 0.0861],\n","        [-0.5760],\n","        [ 0.2032],\n","        [ 1.1331],\n","        [-3.3764]])\n","torch.return_types.sort(\n","values=tensor([[-3.3764],\n","        [-1.4611],\n","        [-0.5760],\n","        [ 0.0861],\n","        [ 0.2032],\n","        [ 1.1331]]),\n","indices=tensor([[5],\n","        [0],\n","        [2],\n","        [1],\n","        [3],\n","        [4]]))\n","torch.return_types.sort(\n","values=tensor([[ 1.1331],\n","        [ 0.2032],\n","        [ 0.0861],\n","        [-0.5760],\n","        [-1.4611],\n","        [-3.3764]]),\n","indices=tensor([[4],\n","        [3],\n","        [1],\n","        [2],\n","        [0],\n","        [5]]))\n"]}],"source":["a=torch.randn(6,1)\n","print(a)\n","print(torch.sort(a,dim=0)) # dim안주면 값 이상하게나온다(0번째 값을 기준으로 정렬한다.)\n","print(torch.sort(a,dim=0,descending=True)) # 내림차순\n","# torch.sort(a)와 a.sort()는 같은 결과 "]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1672662360162,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"KJUEZtB4rglS","outputId":"9d3e5b96-ff4d-4f6c-d884-7a903e875eec"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.1418,  0.2250,  0.1637, -3.2711],\n","        [ 0.1321, -0.3229, -0.5249, -0.0862],\n","        [ 0.5705,  0.0555, -0.6887, -1.4121]])\n","tensor(-5.3010)\n","tensor([-3.0242, -0.8019, -1.4748])\n","tensor([[-3.0242],\n","        [-0.8019],\n","        [-1.4748]])\n","tensor(-0.4418)\n","tensor([-0.7561, -0.2005, -0.3687])\n","tensor([[-0.7561],\n","        [-0.2005],\n","        [-0.3687]])\n","tensor(1.0295)\n","tensor([[-3.0242],\n","        [-0.8019],\n","        [-1.4748]])\n","tensor([[-0.7561],\n","        [-0.2005],\n","        [-0.3687]])\n","tensor(1.0295)\n"]}],"source":["A=torch.randn(3,4)\n","print(A)\n","print(torch.sum(A))\n","print(torch.sum(A,dim=1))\n","print(torch.sum(A,dim=1,keepdims=True))  # keepdims -> 기존 차원 유지\n","print(torch.mean(A))\n","print(torch.mean(A,dim=1))\n","print(torch.mean(A,dim=1,keepdims=True))\n","print(torch.std(A)) # standard deviation 표준 편차\n","\n","print(A.sum(dim=1, keepdims=True))\n","print(A.mean(dim=1, keepdims=True))\n","print(A.std())"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1672662360162,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"CmuensVlsERE","outputId":"9d6122d1-9acb-40a5-ad6a-9c6ec18cf3e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3, 4, 2, 3, 2, 2, 3, 1, 2, 1, 2, 3])\n","torch.Size([12])\n","tensor([[[3, 4, 2],\n","         [3, 2, 2]],\n","\n","        [[3, 1, 2],\n","         [1, 2, 3]]])\n","3\n"]}],"source":["A=torch.randint(1,5,size=(12,)) # 1부터 5미만 12개 정수 (1 차원은 (N,) 과 같이 표현)\n","print(A)\n","print(A.shape)\n","\n","B=A.reshape(2,2,3) # 3부터 채우고 2 다음에 2를 채운다 -> 뒤에서 부터 채운다\n","print(B)\n","print(B.ndim) # 3 차원 행렬이다"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1672662360162,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"VdXu_OZ1sgNK","outputId":"f5d54411-75aa-4033-802a-30178110e1c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(9)\n","tensor([[9]])\n","tensor([[9]])\n","tensor([[9]])\n","tensor([[9]])\n","torch.Size([4, 3, 6])\n","torch.Size([4, 6, 3])\n","torch.Size([6, 3, 4])\n"]}],"source":["a=torch.tensor([1,2,3])\n","b=torch.tensor([2,2,1])\n","print(torch.sum(a*b))\n","\n","a=a.reshape(3,1)\n","b=b.reshape(3,1)\n","\n","# 2차원일떄\n","print(a.T@b) # 방법1\n","print(a.t()@b) # 방법2\n","print(a.transpose(1,0)@b) # 방법3\n","print(a.permute(1,0)@b) # 방법4 , permute(0번째 dim,1번째 dim,2번째 dim) 만약 permute(0,1,2)하면 그대로이고 permute(0,2,1)하면 행,열이 바뀐다.\n","\n","# 3차원일때 permute사용법\n","A=torch.randn(4,3,6)\n","print(A.shape)\n","print(A.permute(0,2,1).shape)\n","print(A.transpose(0,2).shape) # transpose는 둘끼리 자리 바꾸기만 가능"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1672662360163,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"aYsTsVD7sjmj","outputId":"3f037a1b-6d7f-481b-dccf-0420cf2981d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","        18, 19])\n","tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19]])\n","torch.Size([4, 5])\n","torch.Size([2, 5, 2])\n","torch.Size([2, 2, 5])\n","torch.Size([1, 20])\n","torch.Size([20, 1])\n"]}],"source":["A=torch.arange(20)\n","print(A)\n","print(A.reshape(4,5))\n","print(A.reshape(4,-1).shape) # 4개 행이 될 수 있도록 열의 수를 맞춰라\n","print(A.reshape(2,5,-1).shape)\n","print(A.reshape(2,-1,5).shape)\n","print(A.reshape(1,-1).shape) # 2차원 행 벡터\n","print(A.reshape(-1,1).shape) # 2차원 열 벡터"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1672662360163,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"VFBBHxp2zFyr","outputId":"8e770adf-89b6-49c6-f1db-641e45ec3e5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 3, 4, 5, 6])\n","torch.Size([4, 5, 6])\n","torch.Size([4, 5, 6])\n","torch.Size([2, 3, 4, 5])\n","torch.Size([2, 3, 4, 5])\n","torch.Size([3, 4, 6])\n","torch.Size([3, 4, 6])\n"]}],"source":["x=torch.randn(2,3,4,5,6)\n","print(x.shape)\n","print(x[1,2,:,:,:].shape) \n","print(x[1,2,...].shape) # x[1, 2, …] 는 x[1, 2, :, :, :] 와 같습니다.\n","print(x[:,:,:,:,3].shape)\n","print(x[...,3].shape) # x[…, 3] 는 x[:, :, :, :, 3] 와 같습니다.\n","print(x[1,:,:,3,:].shape)\n","print(x[1,...,3,:].shape) # x[1, …, 3, :] 는 x[1, :, :, 3, :] 와 같습니다."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1672662360163,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"VsBKRAiJzbBz","outputId":"c625e82e-844d-4186-bd72-084f9f48c278"},"outputs":[],"source":["A=torch.ones((2,3,4))\n","B=torch.zeros((2,3,4))\n","\n","\n","# print(A)\n","# print(B)\n","\n","# vstack과 hstack은 쓰지말기 -> 차원 많아지면 헷갈림 -> 밑에있는 cat을 쓰기\n","# C=torch.vstack([A,B]) # vstack은 dim=0\n","# D=torch.hstack([A,B]) # hstack은 dim=1\n","\n","# cat\n","E=torch.cat([A,B],dim=0) # cat =  concatenate : 연결하다, # numpy의 block과 비슷한 역할\n","                        # dim=0 -> 0번째 dim에 쌓는다.   \n","F=torch.cat([A,B],dim=1)\n","G=torch.cat([A, B],dim = 2)\n","\n","# print(C)\n","# print(D)\n","\n","# print(E)\n","# print(G)\n","# print(G)\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1672662360163,"user":{"displayName":"혁펜하임","userId":"13709511177929718136"},"user_tz":-540},"id":"EOsKnQTvzeAL","outputId":"279b4a54-b754-473e-8ab3-d4e27eb64507"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 1, 1, 3, 1, 1, 4, 1])\n","torch.Size([3, 4])\n"]}],"source":["A = torch.randn(1,1,1,3,1,1,4,1)\n","# print(A)\n","print(A.shape)\n","print(A.squeeze().shape)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 4])\n","torch.Size([3, 1, 4])\n","torch.Size([3, 4, 1])\n","torch.Size([1, 3, 4])\n","torch.Size([3, 1, 4])\n","torch.Size([3, 4, 1])\n"]}],"source":["# unsqueeze -> squeeze랑 반대\n","\n","A = torch.randn(3,4)\n","print(A.unsqueeze(dim=0).shape)\n","print(A.unsqueeze(dim=1).shape)\n","print(A.unsqueeze(dim=2).shape)\n","\n","print(A.reshape(1,3,4).shape)\n","print(A.reshape(3,1,4).shape)\n","print(A.reshape(3,4,1).shape)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]])\n","torch.Size([2, 3, 4])\n"]}],"source":["# unsqueeze응용\n","A=torch.ones(3,4)\n","B=torch.zeros(3,4)\n","\n","A=A.unsqueeze(dim=0)\n","B=B.unsqueeze(dim=0)\n","C=torch.cat([A,B],dim=0) # 2차원을 행렬 2개를 가지고 3차원으로 만들기\n","print(C)\n","print(C.shape)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[100,   2],\n","        [  3,   4]])\n","tensor([[100,   2],\n","        [  3,   4]])\n","tensor([[100,   2],\n","        [  3,   4]])\n","tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["A=torch.tensor([[1,2],[3,4]])\n","B=A # 주소가 같음\n","B[0,0]=100\n","\n","print(B)\n","print(A)\n","\n","A=torch.tensor([[1,2],[3,4]])\n","B=A.clone() # 주소가 다름\n","B[0,0]=100\n","\n","print(B)\n","print(A)\n"]},{"cell_type":"markdown","metadata":{},"source":["## @에 대해 좀만 더 "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 10])\n"]}],"source":["# 2차원일때\n","A=torch.randn(5,7)\n","B=torch.randn(7,10)\n","C=A@B\n","print(C.shape)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 5, 10])\n"]}],"source":["# 3차원일때 -> 각 채널마다 @해준다 \n","A=torch.randn(32,5,7)\n","B=torch.randn(32,7,10)\n","C=A@B\n","print(C.shape) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 7, 10])\n","torch.Size([32, 5, 10])\n","torch.Size([32, 5, 10])\n","tensor(9.5367e-07)\n"]}],"source":["# repeat\n","A=torch.randn(32,5,7) # 3차원\n","B=torch.randn(7,10)   # 2차원\n","\n","print(B.repeat(32,1,1).shape) # (1,7,10)으로 만든 후 32번 반복\n","\n","C=A@B.repeat(32,1,1)\n","D=A@B # -> A,B형태달라도 작은차원을 큰차원에맞게 맞춰서 행렬곱해준다.\n","\n","print(C.shape)\n","print(D.shape)\n","\n","print((C-D).abs().max()) # 별로 차이 안난다 -> C와 D는 같다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 3])\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","tensor([[[[1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6]]],\n","\n","\n","        [[[1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6]]],\n","\n","\n","        [[[1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6],\n","          [1, 2, 3, 1, 2, 3],\n","          [4, 5, 6, 4, 5, 6]]]])\n","torch.Size([3, 1, 6, 6])\n"]}],"source":["# repeat 자세히\n","# A=torch.rand(2,3)\n","A=torch.tensor([1,2,3,4,5,6]).reshape(2,3)\n","print(A.shape)\n","A_repeat = A.repeat(3,1,3,2) \n","\n","print(A)\n","print(A_repeat)\n","print(A_repeat.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## numpy <-> torch 왔다갔다 가능"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'>\n","<class 'numpy.ndarray'>\n"]}],"source":["import numpy as np\n","import torch\n","\n","a=np.array([1,2,3])\n","b=torch.tensor([1,2,3])\n","\n","# numpy를 tensor로 변경하는 방법\n","A = torch.tensor(a) # 방법1 -> 많이 쓰는 방법 \n","\n","# A = torch.from_numpy(a) # 방법 2:  함수사용\n","print(type(A))\n","\n","# tensor를 numpy로 변경하는 방법\n","B=np.array(b) # 방법 1\n","B=b.numpy() # 방법 2 -> 많이 쓰는 방법\n","\n","print(type(B))\n"]},{"cell_type":"markdown","metadata":{},"source":["### 딥러닝을 가능하게 한 autograd(자동미분) -- 중요!"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.], requires_grad=True)\n","True\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) # requires_grad=True -> 미분할것이라는 뜻 , 1.(1.0의미) 대신 1넣으면 에러 -> float이여야 해서\n","print(x)\n","print(x.requires_grad)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.], grad_fn=<PowBackward0>)\n","x.grad :  None\n","None\n","x.grad :  tensor([2.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","y=x**2\n","print(y) # PowBackward가 붙어있다!-> 미분되는것\n","\n","print('x.grad : ',x.grad)\n","\n","y.backward() # -> 미분해준것, y를 미분한 2x의 x값에 1을 대입한 것이 x.grad라는 attribute에 넣어둔다.\n","\n","print('x.grad : ',x.grad) # x.grad -> y를 미분한 2x의 x값에 1을 대입한 gradient 값\n","\n","# numercical gradient(미분 정의식 통해 미분)가 아니고 미분 공식을 미리 정해준것(ex)x**n미분 -> n*x**n-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0)\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","y=x.argmax()\n","print(y) # grad_fn 안붙어있다 -> y함수가 grad_fn에 안들어있거나 미분이 불가능한것\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.], grad_fn=<PowBackward0>)\n","tensor([3.], grad_fn=<MulBackward0>)\n","tensor([6.])\n","tensor([3.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","y=x**2\n","print(y)\n","# y.retain_grad() # 이걸 하면 y.grad도 볼수 있다.(y.requires_grad=True하면 에러 -> y는 leaf variable이 아니기 때문)\n","z=3*y\n","print(z) # MulBackward0가 붙어있다!(마지막 연산의 grad_fn만 나타나는거지 실제론 메모리에 이전 grad_fn들 저장돼있다. )\n","\n","# chain rule통해 미분하는 것을 backward()통해 구현해놓았다.\n","z.backward() # z를 x로 미분한다 (requires_grad=True로 돼있는 애들로 미분하는것)\n","\n","print(x.grad)\n","# print(y.grad) \n","# -> y는 leaf Tensor(requires_grad=True 돼있는거)가 아니다 -> Warning이 나온다.-> y.retain_grad()코드를 추가해서 구해줄수있다.(위의 주석 코드)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","y=x**2\n","z=3*y\n","\n","y.backward() # 이렇게 하면 y에서부터 뒤로 넘어감(backward!)\n","print(x.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2.], grad_fn=<AddBackward0>)\n","tensor([8.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","a=x**2\n","b=a+1\n","print(b)# AddBackward0가 붙어있다!(가장 '마지막'에 연산되는게 붙어있다(여기선 +))\n","c=b**2\n","c.backward()\n","print(x.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3.], grad_fn=<AddBackward0>)\n","tensor([4.])\n","tensor([2.])\n"]}],"source":["# 여러개의 변수일때 (편미분 일어남)\n","x=torch.tensor([1.],requires_grad=True) \n","y=torch.tensor([1.],requires_grad=True)\n","z=2*x**2+y**2\n","\n","print(z)\n","z.backward()\n","print(x.grad)\n","print(y.grad)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.], grad_fn=<MulBackward0>)\n","tensor([2.])\n","tensor([1.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","y=torch.tensor([1.],requires_grad=True)\n","z=y*x**2\n","\n","print(z)\n","z.backward()\n","print(x.grad)\n","print(y.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(14., grad_fn=<SumBackward0>)\n","tensor(14., grad_fn=<SumBackward0>)\n","tensor([2., 4., 6.])\n"]}],"source":["x=torch.tensor([1.,2.,3.],requires_grad=True) # x1 = 1, x2 = 2, x3 = 3이라고 하자\n","y=torch.sum(x**2) # x1**2+x2**2+x3**2 , \n","print(y) \n","y.backward() # y=x**2하면 에러 ->  y를 backward()하려면 y는 스칼라여야한다.\n","\n","print(y)\n","print(x.grad) # 스칼라를 벡터로 미분"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.])\n"]}],"source":["x=torch.tensor([1.],requires_grad=True) \n","# ~~~코드\n","x.requires_grad=False # transfer learning(전이학습)할때 필요\n","# x=x.detach() # x.requires_grad=False대신에 이렇게도 쓸수있다(많이쓰임)\n","\n","y=x**2\n","print(y)\n","\n","x=x.detach()\n","# y.backward() -> error!"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","tensor([1.])\n","---------------------------\n","True\n","tensor([1.], grad_fn=<PowBackward0>)\n"]}],"source":["# detach와 torch.no_grad(중요!!!!!!!!!!!!111)\n","x=torch.tensor([1.],requires_grad=True)\n","# chain rule을 위해 계속 grad_fu를 update하니까 grad_fu 잠시 계산 안하고 싶을때 torch.no_grad\n","# 모델 테스트 시에는 불필요하게 grad_fu를 메모리 저장할 필요가 없기 때문!( 훈련할땐 미분이 필요하지만 테스트할때는 필요가 없기 떄문이다.)\n","with torch.no_grad(): # 이 안에서는 gradient function을 계산하지 않는다.(requires_grad를 만지지 않고)\n","    y=x**2\n","    print(x.requires_grad)\n","    print(y)\n","\n","print('---------------------------')\n","# y.backward() error!\n","print(x.requires_grad)\n","y=x**2\n","# y.backward() # -> 정상작동\n","print(y)\n","# print(x.grad) # -> 위에 정상작동 확인"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 교육자료용 (밑의코드는 anaconda 환경에서 에러 -> 구글 코렙에서 생성함)\n","import torch\n","from torchviz import make_dot\n","\n","x=torch.tensor([1.,2.],requires_grad=True)\n","make_dot(x) # 박스안의 숫자 -> 변수개수\n","make_dot(x**2)\n","make_dot(x**2+1)\n","# print((x**2+1)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x=torch.tensor([0.],requires_grad=True)\n","y=torch.abs(x)\n","\n","make_dot(y)\n","# y.backward()\n","# print(x.grad) # -> 원래 절댓값은 미분 안되는데 0일때만 그냥 예외처리 해놓았다."]},{"cell_type":"markdown","metadata":{},"source":["### 간단한 인공신경만 만들기"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameter containing:\n","tensor([[0.1913]], requires_grad=True)\n","Parameter containing:\n","tensor([-0.2437], requires_grad=True)\n","tensor([-0.0525], grad_fn=<ViewBackward0>)\n","torch.Size([1])\n","torch.Size([1, 1])\n","tensor([-0.0525], grad_fn=<AddBackward0>)\n"]}],"source":["import torch\n","from torch import nn\n","\n","x=torch.tensor([1.])\n","model = nn.Linear(1,1) # 입력 node 한 개, 출력 node 한 개인 layer 만듦\n","\n","print(model.weight) # 만들면서 initialize 함\n","print(model.bias)\n","\n","y = model(x) # 여기선 입력이 그대로 출력으로 나가기 activation function은 linear activation\n","print(y)\n","\n","\n","y = x @ model.weight + model.bias\n","print(y)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameter containing:\n","tensor([[ 0.5546],\n","        [-0.1674],\n","        [-0.6937]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.2915, -0.3864,  0.0387], requires_grad=True)\n","Parameter containing:\n","tensor([[0.4192, 0.0455, 0.0406]], requires_grad=True)\n","Parameter containing:\n","tensor([0.4560], requires_grad=True)\n","tensor([ 0.8461, -0.5537, -0.6550], grad_fn=<ViewBackward0>)\n","tensor([0.7590], grad_fn=<ViewBackward0>)\n","tensor([0.7590], grad_fn=<AddBackward0>)\n"]}],"source":["fc1 = nn.Linear(1,3) # fully-connected\n","fc2 = nn.Linear(3,1)\n","\n","print(fc1.weight)\n","print(fc1.bias)\n","print(fc2.weight)\n","print(fc2.bias)\n","\n","x=torch.tensor([1.])\n","x=fc1(x)\n","print(x)\n","x=fc2(x)\n","print(x)\n","\n","x=torch.tensor([1.])\n","y = (x@fc1.weight.T+fc1.bias)@fc2.weight.T+fc2.bias\n","# nn.Linear 는 개x채x행x열에서 \"채\" 형태로 (1D data) 들어오길 기대하는 녀석이다.\n","# 즉, 노드 하나가 곧 한 채널의 의미한다.\n","# 따라서, 데이터 여러개를 통과시키고 싶다면 개x채 의 형태로 줘야 함\n","# why T? weight도 개x채 형태로 만들기 위함!\n","# 일단, weight shape 개x채에서 채는 (대부분)무조건 앞에 거 채널 개수와 맞추셈!\n","# 예를 들어, nn.Linear(2,3) 이면 앞에거 채널 개수는 2 따라서 ?x2 인데\n","# 두 채널 값을 가지고 3개의 노드를 만드는 거라서 3x2 가 된다!\n","\n","print(y)\n","# input size: 1\n","# fc1.weight.T size: 1x3\n","# fc1.bias size: 3\n","# fc2.weight.T size: 3x1\n","# fc2.bias size: 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fc1 = nn.Linear(1,3)\n","fc2 = nn.Linear(3,1)\n","\n","x=torch.tensor([1.])\n","x=fc1(x)\n","print(x)\n","x=fc2(x)\n","print(x)\n","\n","model = nn.Sequential(fc1, fc2) # layer 풀칠\n","x=torch.tensor([1.])\n","print(model(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = nn.Sequential(nn.Linear(2,5), # 여기는 채,채\n","                      nn.Linear(5,10),\n","                      nn.Linear(10,3))\n","\n","x=torch.randn(2)\n","print(x)\n","print(model(x))\n","\n","x=torch.randn(1,2)\n","print(x)\n","print(model(x))\n","\n","x=torch.randn(5,2) # 개x채 => 두 개의 채널 값(키, 몸무게)을 가지는 데이터(사람) 5개를 통과시킴\n","print(x)\n","print(model(x))\n","\n","x=torch.randn(2,3,1,4,5,2)\n","print(model(x).shape)"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.4778, 0.5248, 0.4686],\n","        [0.4776, 0.5248, 0.4684],\n","        [0.4804, 0.5227, 0.4671],\n","        [0.4809, 0.5199, 0.4635],\n","        [0.4812, 0.5213, 0.4658]], grad_fn=<SigmoidBackward0>)\n"]}],"source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(2,5) # xrk (5,2) -> 2개 '채널(특성 이라고 봐도 무방)' 가지니 2로 시작\n","        self.fc2 = nn.Linear(5,10)\n","        self.fc3 = nn.Linear(10,3)\n","        self.sig1 = nn.Sigmoid() # activation function으로 시그모이드함수 사용\n","        self.sig2 = nn.Sigmoid()\n","        self.sig3 = nn.Sigmoid()\n","\n","    def forward(self,x):\n","        x=self.fc1(x)\n","        x=self.sig1(x)\n","        x=self.fc2(x)\n","        x=self.sig2(x)\n","        x=self.fc3(x)\n","        x=self.sig3(x)\n","        \n","        return x\n","\n","model = MyModel()\n","x=torch.randn(5,2) # 2개채널 가지는 데이터 5개\n","y = model(x) # y = model.forward(x)과 동일\n","print(y)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MyModel(\n","  (fc1): Linear(in_features=2, out_features=5, bias=True)\n","  (fc2): Linear(in_features=5, out_features=10, bias=True)\n","  (fc3): Linear(in_features=10, out_features=3, bias=True)\n","  (sig1): Sigmoid()\n","  (sig2): Sigmoid()\n","  (sig3): Sigmoid()\n",")\n","Parameter containing:\n","tensor([[ 0.6755,  0.2762],\n","        [-0.0646,  0.0959],\n","        [ 0.5820, -0.2719],\n","        [-0.2604, -0.0969],\n","        [-0.5298, -0.4252]], requires_grad=True)\n","Parameter containing:\n","tensor([-0.2883, -0.3166,  0.4077,  0.1259, -0.3441, -0.1134, -0.4209,  0.1919,\n","        -0.0547,  0.4428], requires_grad=True)\n"]}],"source":["print(model)\n","print(model.fc1.weight) # 만든 모델의 fc1의 weight확인\n","print(model.fc2.bias) "]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.4651, 0.3517, 0.5019],\n","        [0.4647, 0.3528, 0.5004],\n","        [0.4638, 0.3545, 0.4993],\n","        [0.4652, 0.3517, 0.5014],\n","        [0.4637, 0.3546, 0.4988]], grad_fn=<SigmoidBackward0>)\n"]}],"source":["# sequential로 묶기\n","\n","class MyModel2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Sequential(\n","        nn.Linear(2,5),\n","        nn.Sigmoid() ,\n","        nn.Linear(5,10),\n","        nn.Sigmoid(),\n","        nn.Linear(10,3) ,\n","        nn.Sigmoid()\n","\n","        )\n","\n","\n","    def forward(self,x):\n","        x = self.linear(x)\n","        return x\n","\n","model2 = MyModel2()\n","x=torch.randn(5,2) \n","y = model2(x)  # PyTorch에서는 __init__, forward 메서드만이 모델이 호출될 때 실행되는 메서드, 만약 위에서 test메서드가 따로 존재한다면 result = model.test(x)라고 해줘야한다.\n","print(y)"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MyModel2(\n","  (linear): Sequential(\n","    (0): Linear(in_features=2, out_features=5, bias=True)\n","    (1): Sigmoid()\n","    (2): Linear(in_features=5, out_features=10, bias=True)\n","    (3): Sigmoid()\n","    (4): Linear(in_features=10, out_features=3, bias=True)\n","    (5): Sigmoid()\n","  )\n",")\n","Parameter containing:\n","tensor([[ 0.2076, -0.2912],\n","        [ 0.5510, -0.0655],\n","        [ 0.1975,  0.3459],\n","        [ 0.0385, -0.1061],\n","        [ 0.2308,  0.2851]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.0719, -0.2310, -0.0070], requires_grad=True)\n"]}],"source":["print(model2)\n","print(model2.linear[0].weight) # 0번째꺼 weight , model2.linear[0] -> 0번째꺼(이때 0번째꺼는 nn.Linear(2,5)이지 nn.Linear(2,5)와 nn.Sigmoid()를 합친게 아니다.)\n","# model2.linear[-1].weight -> error -> why? -> sigmoid함수는 파라미터가 없다. 하지만 activationn function이 PReLU(parametric ReLU)경우는 음수쪽 기울기를 파라미터로 둔다!! -> 파라미터 loss감소하는 쪽으로 학습된다.\n","print(model2.linear[-2].bias)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.2076, -0.2912],\n","         [ 0.5510, -0.0655],\n","         [ 0.1975,  0.3459],\n","         [ 0.0385, -0.1061],\n","         [ 0.2308,  0.2851]], requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.4625, -0.2321,  0.0373,  0.6399,  0.4151], requires_grad=True),\n"," Parameter containing:\n"," tensor([[ 0.2412, -0.3765, -0.3015,  0.0688, -0.1171],\n","         [-0.0597, -0.4161, -0.3212,  0.3512, -0.2234],\n","         [ 0.1772, -0.2437,  0.2005, -0.1854, -0.0516],\n","         [ 0.1190, -0.3116, -0.2440, -0.2048, -0.1024],\n","         [-0.3911, -0.0199,  0.0078, -0.4410, -0.1776],\n","         [ 0.1301, -0.4373,  0.3108, -0.1286, -0.1032],\n","         [ 0.2631, -0.3913,  0.1121, -0.1352, -0.3090],\n","         [ 0.2813,  0.2058,  0.0373,  0.2405, -0.2470],\n","         [ 0.1284,  0.2427,  0.1862, -0.1171,  0.0911],\n","         [ 0.1017, -0.2819,  0.2356,  0.4032, -0.0958]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.0411,  0.3357,  0.1673, -0.2713, -0.0692, -0.2996, -0.4342,  0.0570,\n","         -0.1557, -0.4238], requires_grad=True),\n"," Parameter containing:\n"," tensor([[-0.2099, -0.1466, -0.2274,  0.0768, -0.2574,  0.2141,  0.0849,  0.0355,\n","          -0.1338,  0.1440],\n","         [ 0.2960,  0.0498, -0.2869, -0.2873, -0.2226, -0.0114,  0.3089, -0.2931,\n","          -0.1401, -0.1301],\n","         [ 0.0086, -0.2021,  0.2868, -0.0474, -0.1971,  0.1062, -0.0186, -0.1156,\n","           0.0566,  0.0827]], requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.0719, -0.2310, -0.0070], requires_grad=True)]"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["# 모델의 파라미터 확안\n","list(model2.parameters()) # model.parameters() -> generate 타입 -> list로 바꿔주기-----------------------> 나중에 자세히 하고 정리하기"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["108\n"]}],"source":["# 파라미터 수 구하기\n","num = sum([p.numel() for p in model.parameters() if p.requires_grad]) # A.numel()) # 전체 성분의 수 , if p.requires_grad -> '학습' 파라미터의 개수만 세고 싶을때 \n","print(num)"]},{"cell_type":"markdown","metadata":{},"source":["## weight initialization( 그냥 SGG 읽기)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 내 필기용 : He initialization 이 공식 문서랑 다르다? => paper에 맞게 구현됐고 torch 공식 문서만 틀림( 지금은 고쳐졌을수도 있음)\n","# initial weight를 직접 원하는 형태로 바꿔주기\n","import torch\n","from torch import nn\n","\n","Fin=5000\n","Fout=1000\n","w = torch.zeros(141, Fin)\n","nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu') # forward pass에서 값의 범위를 유지시켜주기 위함\n","print(w.std())\n","print(torch.sqrt(torch.tensor(2/Fin)))\n","w = torch.zeros(Fout, 212)\n","nn.init.kaiming_uniform_(w, mode='fan_out', nonlinearity='relu') # backward pass에서 값의 범위를 유지시켜주기 위함\n","print(w.std())\n","print(torch.sqrt(torch.tensor(2/Fout)))\n","\n","# CNN?\n","N=32\n","C=64\n","H=6\n","W=10\n","w = torch.zeros(N,C,H,W)\n","nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n","print(w.std())\n","print(torch.sqrt(torch.tensor(2/(C*H*W))))\n","\n","w = torch.zeros(N,C,H,W)\n","nn.init.kaiming_uniform_(w, mode='fan_out', nonlinearity='relu')\n","print(w.std())\n","print(torch.sqrt(torch.tensor(2/(N*H*W))))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNjRfOZpQnp6ObWNotawq9w","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}
